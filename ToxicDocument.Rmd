---
title: "Toxic Text Classification"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction 

Developing a classification algorithm for wikipedia comments to detect if they are toxic, severe toxic, obscene, threat, insult and/or identity hate. The data set is part of a Kaggle competition. Learnings and inspiration and taken from <https://www.tidytextmining.com>.

## Loading needed packages
```{r initialization, results='hide', message=FALSE, warning=FALSE}
library(dplyr)
library(stringr)
library(tm)
library(tidytext)
library(ggplot2)
library(SnowballC)

```
## Loading data
```{r loading data}
df_train <- read.csv("data/train.csv", sep = ",", stringsAsFactors = F)
df_test <-  read.csv("data/test.csv", sep = ",", stringsAsFactors = F)

head(df_train)
```
## Data preparation
Create new feature for simplifying first classification step. Whether or not the wikipedia comment is toxic or not. This is only possible for the training data set.

```{r spamham}
df_train$rowSum <- rowSums(df_train[,3:8])
df_train$classification1 <- ifelse(df_train$rowSum==0,'ham', 'spam')
```

Creating a tidytext format for the data set. Removing stop words, numbers, small letters only. A combintation of tidytext, dplyr and SnowballC
```{r tidytext , message=FALSE, warning=FALSE}
tidy_comment <- df_train %>%
  select(id,comment_text, classification1) %>%
  rename(word = comment_text) %>%
  filter(!str_detect(word,"[0-9]")) %>%
  unnest_tokens(word,word)%>%
  mutate(word = wordStem(word)) %>%
  anti_join(stop_words)

head(tidy_comment,15)
```



